{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: **Design an End-to-End AI Voice Assistance Pipeline üéôÔ∏èü§ñ**"
      ],
      "metadata": {
        "id": "NEGnQz6quDs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Objective:**\n",
        "* Design a pipeline that takes a voice query command, converts it into text, uses a Large Language Model (LLM) to generate a response, and then converts the output text back into speech.\n",
        "* The system should have low latency, Voice Activity Detection (VAD), restrict the output to 2 sentences, and allow for tunable parameters such as pitch, male/female voice,\n",
        "and speed.\n"
      ],
      "metadata": {
        "id": "nhggk5b-uG5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S629NjvJukTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this assignment, we‚Äôll create a seamless pipeline that transforms voice commands into text, generates an AI response using a Large Language Model (LLM), and then converts that response back into speech.\n",
        "* Our system will incorporate the following components:\n",
        "1. Voice Input Processing (Transcription) üé§\n",
        "2. Language Model Response (LLM) üìùüß†\n",
        "3. Text-to-Speech Conversion (TTS) üó£Ô∏èüîä\n"
      ],
      "metadata": {
        "id": "aPhYZ6TzuVeM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koU3lDkNuyS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtEDsbpVuy8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Voice Input Processing (Transcription) üé§üîç:\n",
        "* Utilize an Automatic Speech Recognition (ASR) model (such as Whisper) to transcribe voice queries into text.\n",
        "* Handle audio preprocessing, resampling, and stereo-to-mono conversion."
      ],
      "metadata": {
        "id": "fsaR-Oaiu0_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc9txldr9w4H",
        "outputId": "bb610816-b042-4e2f-bb16-509e8eb34d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-b8fi8pf2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-b8fi8pf2\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=d42cce1f94a10fe0841a0efc8a314268dee59d690790eb71103cbc3678bc5dd7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hzbre36w/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "ye\n",
            "yes\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tag98EcM96r3",
        "outputId": "1d4c80df-4b5d-4fc2-eef9-04a27199d951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting webrtcvad\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/66.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: webrtcvad\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp310-cp310-linux_x86_64.whl size=73459 sha256=11d12cb072471734c6059ed0dcc66c2ef1442256c9de3310a9a32923992371be\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/2b/84/ac7bacfe8c68a87c1ee3dd3c66818a54c71599abf308e8eb35\n",
            "Successfully built webrtcvad\n",
            "Installing collected packages: webrtcvad\n",
            "Successfully installed webrtcvad-2.0.10\n"
          ]
        }
      ],
      "source": [
        "!pip install webrtcvad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M3qCzHgs97E1",
        "outputId": "39a59c2b-e91e-43a3-a9db-e9a0f910f8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transcription:\n",
            " turnout I'm no snooze And everybody knows I get off the train And the ladies will choose the truth I'm like inception I play with you when you're so sweet I'm no snooze I'm playing no games But don't, don't, don't, don't, don't They confuse no Cause you will lose, yeah Man, now pump, pump, pump, pump, pump And bump it up and back it up like a tonk a truck If you go hard you gotta get on the floor If you're a party, we can step on the floor If you're an animal, then tear up the floor Break the sweat on the floor, yeah, we work on the floor Don't stop, keep it falling, put your drinks up If you're body up and drop it on the floor Let the rhythm change the world on the floor You know we're running to night on the floor You know we're running to night on the floor Of course you're no rocker And men and children feel so Straight to a lady of pain Just a happy day Day is the night away If your life can stay on on the floor Day is the night away Grab somebody drink the money Send them home Send them home leaving, They'll open up Let the rhythm change the world In the middle of the street And come back and make Face Seems like you brought me in the floor Don't stop, Keep it fallin' Keep the rain coming in the floor When you see of my life On the floor, and keep on rockin' my feet up on the floor If you're a criminal, you'll kill it On the floor, steal it quick On the floor, on the floor Don't stop, you've been mumbling what you've dreamed so It's getting ill, it's getting sick on the floor We never quit, we never rest on the floor If I ain't wrong with coming back on the floor Or steal it, Morocco Nothing to it, be some straight to lay New York Vegas to Africa There's the light away if your life can stay on the floor There's the light away from somebody drink from me Would you like more won't you bring it for? No way! Where the world truly goes To Dad, My Brother, In California I'm too late for you to get up the door I'm too late for you to get up the door I'm too late for you to get up the door I'm too late for you to get up the door The dog is like a trunks full of bass on the old school shit They're seven straight down to nine All I need is some back, I'm so drunk and calm And watch it, you don't get drunk and calm Maybe if you're ready for time, I'll get it, I'll get on the floor And I gotta flow with you, let me lie No more lemme just beat me I'll see you when you're sweet man, L.A. Miami, New York Say no more, get on the floor Take your still light away, if your life can stay your long on the floor Take your still light away, grab somebody drink a little more I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor Tonight, we gon' be in on the floor Tonight, we gon' be in on the floor Tonight, we gon' be in on the floor Oh Oh\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"base.en\")\n",
        "\n",
        "# Define the audio processing settings\n",
        "audio_path = \"/content/lofi_chase_on_the_floor(256k).mp3\"  # Replace with your audio file path\n",
        "sampling_rate = 16000  # Target sampling rate\n",
        "\n",
        "# Load and resample the audio\n",
        "waveform, original_sr = torchaudio.load(audio_path)\n",
        "waveform = torchaudio.transforms.Resample(orig_freq=original_sr, new_freq=sampling_rate)(waveform)\n",
        "\n",
        "# Convert stereo to mono if needed\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0).unsqueeze(0)\n",
        "\n",
        "# Save the processed audio to a temporary file\n",
        "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "    temp_path = temp_file.name\n",
        "    torchaudio.save(temp_path, waveform, sampling_rate)\n",
        "\n",
        "# Transcribe the audio using Whisper\n",
        "result = model.transcribe(temp_path, language=\"en\")\n",
        "\n",
        "# Remove the temporary file\n",
        "os.remove(temp_path)\n",
        "\n",
        "# Output the transcribed text\n",
        "print(\"\\nTranscription:\")\n",
        "for line in result['text'].split('\\n'):\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRu73ZJf97J7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Language Model Response (LLM) üìùüß†:\n",
        "* Employ a pre-trained LLM (e.g., GPT-2) to generate a contextually relevant response based on the transcribed text.\n",
        "* Limit the response to a concise and informative output (e.g., two sentences)."
      ],
      "metadata": {
        "id": "OP1-S0F0u9kv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE0Ss9FP97NO",
        "outputId": "5e51799e-5bb2-4392-a13c-118a8c2f7f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUGCWBOE97QL",
        "outputId": "36eff280-b49a-451e-c3f6-d9a3883c2820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response:  turnout I'm no snooze And everybody knows I get off the train And the ladies will choose the truth I'm like inception I play with you when you're so sweet I'm no snooze I'm playing no games But don't, don't, don't, don't, don't They confuse no Cause you will lose, yeah Man, now pump, pump, pump, pump, pump And bump it up and back it up like a tonk a truck If you go hard you gotta get on the floor If you're a party, we can step on the floor If you're an animal, then tear up the floor Break the sweat on the floor, yeah, we work on the floor Don't stop, keep it falling, put your drinks up If you're body up and drop it on the floor Let the rhythm change the world on the floor You know we're running to night on the floor You know we're running to night on the floor Of course you're no rocker And men and children feel so Straight to a lady of pain Just a happy day Day is the night away If your life can stay on on the floor Day is the night away Grab somebody drink the money Send them home Send them home leaving, They'll open up Let the rhythm change the world In the middle of the street And come back and make Face Seems like you brought me in the floor Don't stop, Keep it fallin' Keep the rain coming in the floor When you see of my life On the floor, and keep on rockin' my feet up on the floor If you're a criminal, you'll kill it On the floor, steal it quick On the floor, on the floor Don't stop, you've been mumbling what you've dreamed so It's getting ill, it's getting sick on the floor We never quit, we never rest on the floor If I ain't wrong with coming back on the floor Or steal it, Morocco Nothing to it, be some straight to lay New York Vegas to Africa There's the light away if your life can stay on the floor There's the light away from somebody drink from me Would you like more won't you bring it for? No way! Where the world truly goes To Dad, My Brother, In California I'm too late for you to get up the door I'm too late for you to get up the door I'm too late for you to get up the door I'm too late for you to get up the door The dog is like a trunks full of bass on the old school shit They're seven straight down to nine All I need is some back, I'm so drunk and calm And watch it, you don't get drunk and calm Maybe if you're ready for time, I'll get it, I'll get on the floor And I gotta flow with you, let me lie No more lemme just beat me I'll see you when you're sweet man, L.A. Miami, New York Say no more, get on the floor Take your still light away, if your life can stay your long on the floor Take your still light away, grab somebody drink a little more I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor I'm too late for you to get up the floor Tonight, we gon' be in on the floor Tonight, we gon' be in on the floor Tonight, we gon' be in on the floor Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "# Load pre-trained LLM model\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a tokenizer and set pad_token to eos_token\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Input text from Step 1 (replace with actual transcription)\n",
        "input_text = result['text']  # Ensure `result['text']` contains valid text\n",
        "\n",
        "# Generate LLM response\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
        "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()  # Create attention mask based on padding\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Generate the response with a controlled length\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, num_return_sequences=1)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"LLM Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ubl5R-qRvFsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text-to-Speech Conversion (TTS) üó£Ô∏èüîä:\n",
        "* Convert the LLM-generated response back into speech.\n",
        "* Use a TTS system (e.g., edge-tts) to create an audio output."
      ],
      "metadata": {
        "id": "QsCyPxMzvGSi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KIO60kMc97Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b592f477-9489-4add-e46b-ff40c8c96a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting edge-tts\n",
            "  Downloading edge_tts-6.1.12-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from edge-tts) (3.10.5)\n",
            "Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.10/dist-packages (from edge-tts) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.0->edge-tts) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.0->edge-tts) (3.7)\n",
            "Downloading edge_tts-6.1.12-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: edge-tts\n",
            "Successfully installed edge-tts-6.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install edge-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yIELf33397ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58950cfb-8095-4ea8-cb8a-bcd5d8027a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech saved as output.wav\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import edge_tts\n",
        "# Use edge-tts to convert LLM response to speech\n",
        "async def text_to_speech(text: str, output_file: str):\n",
        "    communicate = edge_tts.Communicate(text, voice='en-US-GuyNeural')\n",
        "    await communicate.save(output_file)\n",
        "\n",
        "# Save the LLM response as speech in a .wav file\n",
        "output_file = \"output.wav\"\n",
        "# Use await instead of asyncio.run() inside an existing event loop\n",
        "await text_to_speech(response, output_file)\n",
        "\n",
        "print(f\"Speech saved as {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNiLNbMYvOSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Additional Considerations:\n",
        "* Implement Voice Activity Detection (VAD) to identify when the user is speaking.\n",
        "* Allow for tunable parameters, such as pitch, male/female voice, and speed.\n",
        "Ensure low latency throughout the pipeline."
      ],
      "metadata": {
        "id": "jBF1OIe6vOnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Voice Activity Detection (VAD) Implementation üîçüéôÔ∏è\n",
        "* Voice Activity Detection (VAD) is crucial for identifying when the user starts and stops speaking, ensuring that the pipeline processes only relevant audio input. You can use the webrtcvad library to implement VAD."
      ],
      "metadata": {
        "id": "8SmyRs9bwFo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webrtcvad\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def detect_voice_activity(waveform, sample_rate, frame_duration=30):\n",
        "    vad = webrtcvad.Vad(1)  # Mode 1 for less aggressive VAD\n",
        "    samples_per_frame = int(sample_rate * frame_duration / 1000)\n",
        "    num_frames = waveform.size(1) // samples_per_frame\n",
        "\n",
        "    voice_activity = False\n",
        "    for i in range(num_frames):\n",
        "        start = i * samples_per_frame\n",
        "        end = start + samples_per_frame\n",
        "        frame = waveform[0, start:end].numpy()\n",
        "\n",
        "        # Ensure the frame is in 16-bit PCM format\n",
        "        frame = (frame * 32767).astype(np.int16)\n",
        "        if vad.is_speech(frame.tobytes(), sample_rate):\n",
        "            voice_activity = True\n",
        "            break\n",
        "\n",
        "    return voice_activity\n",
        "\n",
        "\n",
        "# Process and check audio file\n",
        "waveform, original_sr = torchaudio.load(audio_path)\n",
        "waveform = torchaudio.transforms.Resample(orig_freq=original_sr, new_freq=sampling_rate)(waveform)\n",
        "\n",
        "# Convert stereo to mono if needed\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0).unsqueeze(0)\n",
        "\n",
        "# Check for voice activity\n",
        "if detect_voice_activity(waveform, sampling_rate):\n",
        "    print(\"Voice activity detected. Proceeding with transcription...\")\n",
        "else:\n",
        "    print(\"No voice activity detected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32As9pYX3m7b",
        "outputId": "f5f07244-64d7-4441-9d7b-5c98672c1d80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voice activity detected. Proceeding with transcription...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Generate and Restrict LLM Response"
      ],
      "metadata": {
        "id": "41dbYjuQ0kZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def truncate_response(text, max_sentences=2):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    return ' '.join(sentences[:max_sentences])\n",
        "\n",
        "# Generate LLM response\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
        "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, num_return_sequences=1)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Truncate response to a maximum of 2 sentences\n",
        "truncated_response = truncate_response(response)\n",
        "print(\"LLM Response (Truncated):\", truncated_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9c7JjyrvtXG",
        "outputId": "72874319-ff52-4029-8733-a734759121be"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response (Truncated): The Last I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I'm close, I've been close, lost Troops, I found one Rolls-p Your welcome, and love them to the fizzer Stray to the world in your van This is the half-way dada Day is the light away If your life can stay on the floor Day is the light away Grab somebody drink the water I'll listen to you I'll listen to you I'll listen to you I'll listen to you La la la la la la La la la la la La la la la Tonight we're gonna be here on the floor La la la la la La la la la la la La la la Tonight we're gonna be here on the floor I'm gonna show you how And though you gotta cut your hands on the floor And keep on rocking my feet up on the floor If you're a criminal, you'll kill it I'm the floor still liquid, I'm the floor I'm the floor Don't stop, you've been mumbling what you've dreamed so It's getting ill, it's getting sick on the floor We never quit, we never rest on the floor If I ain't wrong with coming back on the floor We'll feel Morocco, nothing to it either Straight to a lady of Vegas to Africa Dance the light away if your life can stay on the floor Dance the light away from somebody drink from me Let's do it, let's do it, let's do it We're all the world, all the world, all the world Tonight we won't be in all the floors Tonight we won't be in all the floors Tonight we won't be in all the floors Tonight we won't be in all the floors Tonight we won't be in all the floors The note is like a trunk full of bass on the old school street The seven-tray dunkin' down, all I need is some back There's some shrunken car, you want your stupid dunkin' Maybe if you're ready for that I'm in the dead, I get on the floor And I got home with you, let me lie No more lemme just beat me My name ain't me, keep it, I see you when you sweat me I don't aim my aiming, New York, say no more, get on the floor Dance the light away if your life can stay on the floor Dance the light away, grab somebody drink from me Cause I don't aim anything Tonight we won't be in all the floors Tonight we won't be in all the floors Tonight we won't be in all the floors To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor I see that I'm gonna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be it on the floor To make me wanna be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ytHFNhZw3JjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-7hLugAwzDM"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}